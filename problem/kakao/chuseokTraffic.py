import datetime
# 카카오 공채) 추석 트래픽
# 문제 설명 :  https://tech.kakao.com/posts/344
# 특정 1초(Sec) 구간에 가장 많은 수의 요청이 동시에 처리되고 있는 최대 개수를 찾는 문제다. 즉, 특정 시점부터 1초동안 처리되고있는 요청의 최대개수를 센다.
# 예제 3의 경우 20:59:57초부터 20:59:58초까지 총 4개의 요청이 처리되고있음으로 이구간의 초당 최대 처리량은 4이다.
# 참고로 1초마다가 아니라 단지 특정 구간의 1초당 최대 요청 처리 임으로 1~2, 2~3초 마다 요청 처리량 이런식으로 잴 필요가 없다.
# 요청 처리량은 요청이 시작될때, 요청이 종료될때 처리량에 변화가 일어남으로 이 두가지 경우에만 처리량을 재면 된다.

# 예제 관련 추가 설명
# 예를 들어, 로그 문자열 2016-09-15 03:10:33.020 0.011s은 “2016년 9월 15일 오전 3시 10분 33.010초”부터 “2016년 9월 15일 오전 3시 10분 33.020초”까지 “0.011초” 동안 처리된 요청을 의미한다.에서
# 왜 33.010초에서 33.020초인데 0.010초 동안이 아니라 0.011초 동안 처리되었다고 하는 이유는 시간의 길이는 10ms였지만, 시작점과 끝점을 포함하여 세었기 때문에 처리 시간은 11ms(0.011s)라고 기록된다.
# 이것은 로그 시스템의 특정한 계산 방식이며, 일반적인 시간의 뺄셈과는 다른 결과를 낳는 이유로 10부터 20까지 세면 10개가 아니라 11개가 된다.

# 슬라이딩 윈도우로 풀 수 있지만 하루치 로그를 처음부터 끝까지 스캔하기에는 범위가 너무 크고
# ms 단위로 되어 있기 때문에 첫 로그 시각부터 마지막 로그 시각까지 1ms씩 증가시키면서 1000ms 단위의 슬라이딩 윈도우로 풀면 24 * 3600 * 1000 * n * 1000ms 만큼의 연산이 필요하기 때문에 이렇게는 풀 수가 없다.
# 그렇다고 각 로그의 시작 시각부터 마지막 시각까지 1ms 씩 움직이면 time(ms) * n^2 이 되며, time(ms)의 값은 대부분 천 단위 이상이기 때문에 마찬가지로 타임아웃이 발생하여 풀 수가 없다.
# 따라서 슬라이딩 윈도우로 풀이하되 좀 더 효율적인 접근 전략이 필요하다.
# 요청량이 변하는 순간은 각 로그의 시작과 끝이기때문에 각 로그별 2번씩 비교 연산만 수행하는 형태로 풀이할 수 있다.  2 * n^2, 빅오로 정리하면 O(n^2)에 풀 수 있게 된다.
# 빅오에서 제거된 상수항도 매우 작기 때문에 이 경우 무리 없이 문제를 풀 수 있게 되며 C++ 기준으로 10ms를 넘지 않는다.
# 슬라이딩 윈도우 또한 시작 시각과 끝 시각에 따라 윈도우 범위가 달라지므로 투 포인터가 결합된 풀이로 볼 수 있다.
def solution(lines : str ) -> int:

    # 요청의 시작 지점과 끝 지점을 저장할 배열.
    combined_logs = []

    # 입력받은 로그를 하나씩 꺼내서 계산
    for log in lines:
        # 공백 기준으로 분리하여 리스트 logs로 변환
        logs = log.split(' ')
        # logs[0]: 날짜 ("2016-09-15")
        # logs[1]: 시간 ("20:59:57.421")
        # logs[2]: 처리 시간 ("0.351s")
        # 요청이 완료된 시각(logs[0] + logs[1])을 읽어와서, 시간 계산에 용이하도록 초 단위의 실수형 유닉스 타임스탬프로 변환하여 timestamp 변수에 저장한다.
        # 초 단위의 실수형 유닉스 타임스탬프 : 현재 시각이 에포크 시간(1970년 1월 1일 00:00:00)으로부터 얼마나 많은 시간이 경과했는지를 초(Seconds) 단위로 세어서 나타낸 값
        # ex ) 2025-11-12 03:35:20.000은 유닉스 타임 스탬프로 1763000000.000이다.
        timestamp = datetime.datetime.strptime(logs[0] + ' ' + logs[1],
                                               "%Y-%m-%d %H:%ML%S.%f").timestamp()

        # 시작 시간은 1로 종료 시각은 -1로 저장한다.
        combined_logs.append((timestamp, -1))
        # float(logs[2][:-1]): 처리 시간 문자열 ("0.351s")에서 's'를 제거하고 실수형으로 변환한다. 이 시간을 timestamp(요청이 완료된 시각)에서 뺴면 요청이 시작된 시각이다.
        # + 0.001: 시작 시점과 끝 시점을 모두 포함하는 로그 계산법에 따라 0.001s(1ms)를 더해 정확한 시작 시점을 구한다.
        combined_logs.append((timestamp - float(logs[2][:-1]) + 0.001, 1))

    # 현재까지 누적되어 처리 중인 요청의 개수를 저장하는 변수
    accumulated = 0
    # 최소 요청 개수는 1개이므로 최대 요청개수를 저장하는 변수를 1로 초기화한다.
    max_requests = 1

    # lambda x : x[0] : 를 사용하여 x[0]을 기준으로 정렬한다.
    # 즉, 리스트를 시간 기준으로 오름 차순 정렬한다.
    combined_logs.sort(key=lambda x : x[0])

    # 여기서 i는 인덱스 elem1은 위에서 저장한 튜플()이다.
    for i, elem1 in enumerate(combined_logs):
        # 1초 구간 내에 최대 동시 처리 요청 수를 계산하기 위한 변수
        current = accumulated

        # 현재 elem1 이후 발생한 모든 로그를 들고와서 elem2에 넣어 꺼낸다.
        for elem2 in combined_logs[i:]:
            # 현재 로그(elem1)과 이후 로그(elem2)의 시간 차이가 0.999(1000ms,1초)보다 크다면
            if elem2[0] - elem1[0] > 0.999:
                # 해당 elem2는 윈도우 바깥에 있음으로 루프 종료 (1초 안에 없는 요청이라는 뜻)
                break

            # elem2[1] > 0 : elem2[1]은 시작 시점인지 종료 시점인지 알려주는 변수이다. 1이라면 시작시점이다. 종료시점은 무시한다.
            if elem2[1] > 0:
                # current에 $+1$을 더한다. 여기서 elem2[1]은 1이다.
                current += elem2[1]

        # 현재 초당 요청 개수가 최대 요청 개수보다 크다면 갱신.
        max_requests = max( max_requests, current)
        # accumulated은 현재 기준 elem1이전에 실행되었고 elem1 시점 현재도 진행중인 모든 요청을 카운팅하기 위한 변수다.
        # 그럼으로 다음 루프 계산에 쓰이는 변수라고 할 수 있다.
        # 현재 elem1이 시작 지점이라면 다음 1초에도 진행되고 있을 것임으로 +1 , 종료 시점이라면 다음 1초에는 없을 테니 -1
        accumulated += elem1[1]

    return max_requests